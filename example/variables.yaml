# ------------------------ #
# 基础信息配置
# ------------------------ #
# 是否跳过节点物理资源校验
skip_verify_node: false

# 安装模式
  # 在线 online 
  # 离线 offline
  # 应用于可能没有公网的环境
install_mode: online

# 代理设置，若节点不能直接访问公网，可使用下面变量设置代理。
# http_proxy: 
# https_proxy: 
# no_proxy: 192.168.0.0/16,10.0.0.0/8,172.16.0.0/12,127.0.0.1,localhost

# 节点时区
timezone: Asia/Shanghai

# CentOS yum源仓库
  # 基础软件源
# base_yum_repo: http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/
  # epel软件源
epel_yum_repo: http://mirrors.aliyun.com/epel/7/$basearch
  # docker-ce源
docker_yum_repo: https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable
  # kubernetes源
kubernetes_yum_repo: https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-{{ ansible_architecture }}/

# Debian apt源仓库
  # 基础软件源
# base_apt_repo: deb http://mirrors.aliyun.com/{{ host_distribution | lower }}/ {{ host_distribution_release }} main restricted universe multiverse
  # docker-ce源
docker_apt_repo: "deb [arch={{ host_architecture }}] https://mirrors.aliyun.com/docker-ce/linux/{{ host_distribution | lower }} {{ host_distribution_release }} stable"
  # kubernetes源
kubernetes_apt_repo: deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main

# ------------------------ #
# Docker 相关参数配置
# ------------------------ #
# Docker版本
docker_version: 19.03.12

# 国内镜像加速(若不需要加速，请将值删去，键保留)
docker_mirror: 
# - "https://reg-mirror.qiniu.com"
# - "https://hub-mirror.c.163.com"
# - "https://docker.mirrors.ustc.edu.cn"

# 信任的不安全镜像库地址，默认为 Pod 和 Service 网段
docker_insecure_registries:
- "{{ kube_pod_subnet }}"
- "{{ kube_service_subnet }}"

# docker日志相关
docker_log_driver: "json-file"
docker_log_level: "warn"
docker_log_max_size: "10m"
docker_log_max_file: 3

# docker容器存储目录
docker_storage_dir: "/var/lib/docker"

# 并行镜像下载数量
docker_max_concurrent_downloads: 10

# ------------------------ #
# load-balancer 相关参数配置
# ------------------------ #
# 私有云：
#    VIP 负载模式：
#       也就是负载均衡器 + keepalived 模式，比如常用的 haproxy + keepalived。
#       本脚本中负载均衡器有 openresty、nginx、haproxy、envoy 可供选择，设置 lb_mode 即可进行任意切换。
#       设置 lb_kube_apiserver_ip 即表示启用 keepalived，请先与服务器提供部门协商保留一个IP作为 lb_kube_apiserver_ip，
#       一般 lb 节点组中有两个节点就够了，lb节点组中第一个节点为 keepalived 的 master 节点，剩下的都为 backed 节点。
#
#    节点本地负载模式：
#       只启动负载均衡器，不启用 keepalived（即不设置 lb_kube_apiserver_ip），
#       此时 kubelet 链接 apiserver 地址为 127.0.0.1:lb_kube_apiserver_port。
#       使用此模式时请将 lb 节点组置空。
#
# 公有云：
#    不推荐使用 slb 模式，建议直接使用节点本地负载模式。
#    若使用 slb 模式，请先使用节点本地负载模式进行部署，
#    部署成功后再切换至 slb 模式：
#       将 lb_mode 修改为 slb，将 lb_kube_apiserver_ip 设置为购买到的 slb 内网ip，
#       修改 lb_kube_apiserver_port 为 slb 监听端口。
#    再次运行初始化集群脚本即可切换至 slb 模式。
lb_mode: openresty

# 使用负载均衡后集群 apiserver ip
# lb_kube_apiserver_ip: "192.168.56.15"

# 使用负载均衡后集群 apiserver port
lb_kube_apiserver_port: 8443

# 负载均衡器健康检查端口
lb_kube_apiserver_healthcheck_port: 8081

# 启用 ingress NodePort服务的负载均衡 (true/false)
enabel_ingress_nodeport_lb: false
# 启用 ingress tls NodePort服务的负载均衡 (true/false)
enabel_ingress_tls_nodeport_lb: false

# 使用openresty进行apiserver负载时使用的openresty镜像
lb_openresty_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/openresty_openresty:1.17.8.2-alpine

# 使用nginx进行apiserver负载时使用的nginx镜像
lb_nginx_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/nginx:1.17.9-alpine

# 使用haproxy进行apiserver负载时使用的haproxy镜像
lb_haproxy_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/haproxy:2.1-alpine
# haproxy监控绑定端口
lb_haproxy_stats_bind_address: 9090
# haproxy监控访问路径
lb_haproxy_stats_uri: "/stats"
# haproxy监控自动刷新时间（秒）
lb_haproxy_stats_refresh: 10
# haproxy监控用户名
lb_haproxy_stats_user: "admin"
# haproxy监控用户密码
lb_haproxy_stats_password: "admin"
# haproxy负载均衡算法，常见如下：
# "roundrobin": 基于服务器权重的轮询
# "leastconn": 基于服务器最小连接数
# "source": 基于请求源IP地址
# "uri": 基于请求的URI
lb_haproxy_balance_alg: "leastconn"

# 使用haproxy进行apiserver负载时使用的haproxy镜像
lb_envoy_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/envoyproxy_envoy-alpine:v1.13.1
lb_envoy_admin_address_port: 9090

# 使用 vip 负载时使用的 keepalived 镜像
lb_keepalived_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/osixia_keepalived:2.0.20
# keepalived auth_type 的 password
lb_keepalived_password: "d0cker"
# 区分多个 instance 的 VRRP 组播，同网段不能重复，取值在0-255之间
lb_keepalived_router_id: 51

# ------------------------ #
# Etcd 相关参数配置
# ------------------------ #

# Etcd证书过期时间（天）
etcd_certs_expired: 3650
# Etcd根证书过期时间（天）
etcd_ca_certs_expired: 36500
# Etcd使用的镜像
etcd_image: registry.aliyuncs.com/k8sxio/etcd:3.4.3-0
# Etcd 数据根目录
etcd_data_dir: "/var/lib/etcd"
# Etcd 每日备份时间，默认3，即凌晨3点，取值范围0-23
etcd_backup_hour: "3"
# Etcd 每日备份文件保留时长，默认7天
etcd_backup_expiry: "7"

# ------------------------ #
# kubernetes 相关参数配置
# ------------------------ #

# kubernetes证书过期时间（天）
kube_certs_expired: 3650
# kubernetes根证书过期时间（天）
kube_ca_certs_expired: 36500

# 加入集群token
kubeadm_token: "abcdef.0123456789abcdef"

# k8s 集群 master 节点证书配置，可以添加多个ip和域名（比如增加公网ip和域名）
kube_master_external_ip:
- "8.8.8.8"
kube_master_external_domain:
- "kubernetes.io"

# Pod根容器
pod_infra_container_image: registry.aliyuncs.com/k8sxio/pause:3.2

# kubernetes各组件镜像仓库前缀
kube_image_repository: registry.aliyuncs.com/k8sxio

# kubernetes版本
kube_version: 1.18.6

# 集群内部dns域名
kube_dns_domain: cluster.local

# 网段选择：pod 和 service 的网段不能与服务器网段重叠，
# 若有重叠请配置 `kube_pod_subnet` 和 `kube_service_subnet` 变量设置 pod 和 service 的网段，示例参考：
#    如果服务器网段为：10.0.0.1/8
#       pod 网段可设置为：192.168.0.0/18
#       service 网段可设置为 192.168.64.0/18
#    如果服务器网段为：172.16.0.1/12
#       pod 网段可设置为：10.244.0.0/18
#       service 网段可设置为 10.244.64.0/18
#    如果服务器网段为：192.168.0.1/16
#       pod 网段可设置为：10.244.0.0/18
#       service 网段可设置为 10.244.64.0/18
# 集群pod ip段，默认掩码位 18 即 16384 个ip
kube_pod_subnet: 10.244.0.0/18
# 集群service ip段
kube_service_subnet: 10.244.64.0/18
# 分配给节点的 pod 子网掩码位，默认为 24 即 256 个ip，故使用这些默认值可以纳管 16384/256=64 个节点。
kube_network_node_prefix: 24

# node节点最大 pod 数。数量与分配给节点的 pod 子网有关，ip 数应大于 pod 数。
# https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr
kube_max_pods: 110

# NodePort端口范围
kube_service_node_port_range: 30000-32767

# 资源保留相关配置
eviction_hard_imagefs_available: 15%
eviction_hard_memory_available: 100Mi
eviction_hard_nodefs_available: 10%
eviction_hard_nodefs_inodes_free: 5%

# kubernetes组件预留资源
kube_cpu_reserved: 100m
kube_memory_reserved: 256M
kube_ephemeral_storage_reserved: 1G

# # 操作系统守护进程预留资源
# system_reserved_enabled: true
# # 取消注释以覆盖默认值
# system_cpu_reserved: 500m
# system_memory_reserved: 512M
# system_ephemeral_storage_reserved: 10G

# 默认使用kube-proxy的 'iptables' 模式，可选 'ipvs' 模式
kube_proxy_mode: iptables

# Kubelet 根目录
kubelet_root_dir: "/var/lib/kubelet"

## 存入 Etcd 时的 Secret 进行静态加密 
# 仅支持: aescbc, secretbox 或 aesgcm
kube_encryption_algorithm: "aescbc"
# 将Secret数据加密存储到etcd中的配置文件，下面加密码由 head -c 32 /dev/urandom | base64 生成
kube_encrypt_token: "GPG4RC0Vyk7+Mz/niQPttxLIeL4HF96oRCcBRyKNpfM="

## 审计相关配置
# 是否启用审计
kubernetes_audit: false
# 保留审计日志最大天数
audit_log_maxage: 30
# 保留审计日志最大个数
audit_log_maxbackups: 10
# 保留审计日志最大容量（MB）
audit_log_maxsize: 100
# 审计日志文件挂载在主机上的目录
audit_log_hostpath: /var/log/kubernetes/audit
# 审计策略配置文件路径
audit_policy_file: /etc/kubernetes/config/apiserver-audit-policy.yaml
# 自定义审计日志规则 (替换默认的审计规则)
# audit_policy_custom_rules: |
#   - level: None
#     users: []
#     verbs: []
#     resources: []

# 1.10+ admission plugins
kube_apiserver_enable_admission_plugins: 
- NodeRestriction
# - AlwaysPullImages
# - PodSecurityPolicy

# 1.10+ list of disabled admission plugins
kube_apiserver_disable_admission_plugins: []

# kube-controller-manager 标记 kubelet(node) 为不健康的周期
kube_controller_node_monitor_grace_period: 40s
# kube-controller-manager 定期检查 kubelet(node) 状态周期
kube_controller_node_monitor_period: 5s
# kube-controller-manager 判定节点故障，重建 Pod 的超时时间，默认值 5m0s，这里改为了 2m0s
kube_controller_pod_eviction_timeout: 2m0s
# exit 状态的 pod 超过多少会触发 gc，默认值 12500，这里改为了 10
kube_controller_terminated_pod_gc_threshold: 10

## Extra args for k8s components passing by kubeadm
kube_kubeadm_apiserver_extra_args: {}
kube_kubeadm_controller_extra_args: {}
kube_kubeadm_scheduler_extra_args: {}

## Extra control plane host volume mounts
## Example:
# apiserver_extra_volumes:
#  - name: name
#    hostPath: /host/path
#    mountPath: /mount/path
#    readOnly: true
apiserver_extra_volumes: {}
controller_manager_extra_volumes: {}
scheduler_extra_volumes: {}

# ------------------------ #
# 集群插件相关参数配置
# ------------------------ #

# 是否等待插件运行成功
wait_plugins_ready: true

# ------------------------ #
# 集群网络插件相关参数配置
# ------------------------ #

# 是否启用网络组建
network_plugins_enabled: true

# 集群网络插件，目前支持flannel, calico, kube-ovn
network_plugin: "calico"

# calico mtu
calico_veth_mtu: 1440
# calico 相关镜像
calico_typha_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/calico_typha:v3.16.5
calico_cni_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/calico_cni:v3.16.5
calico_node_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/calico_node:v3.16.5
calico_kube_controllers_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/calico_kube-controllers:v3.16.5
calico_pod2daemon_flexvol_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/calico_pod2daemon-flexvol:v3.16.5
# 设置 Felix 日志级别(debug, info, warning, error)
calico_felix_log_level: "warning"
# calicoctl image 地址
calicoctl_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/calico_ctl:v3.16.5

# 设置 flannel 后端
# flannel_backend: "host-gw"
flannel_backend: "vxlan"
# flannel 镜像地址
flannel_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/coreos_flannel:v0.13.0

# kube-ovn master 节点
kube_ovn_master:
- "{{ groups['kube-master'][0] }}"
# kube-ovn 相关镜像
kube_ovn_controller_image: index.alauda.cn/alaudak8s/kube-ovn-controller:v1.0.1
kube_ovn_cni_image: index.alauda.cn/alaudak8s/kube-ovn-cni:v1.0.1
kube_ovn_db_image: index.alauda.cn/alaudak8s/kube-ovn-db:v1.0.1
kube_ovn_node_image: index.alauda.cn/alaudak8s/kube-ovn-node:v1.0.1
kube_ovn_pinger_image: index.alauda.cn/alaudak8s/kube-ovn-pinger:v1.0.1
# 默认网段设置
kube_ovn_default_cidr: "{{ kube_pod_subnet }}"
kube_ovn_default_gateway: "{{ kube_pod_subnet | ipaddr('net') | ipaddr(1) | ipaddr('address') }}"
kube_ovn_node_switch_cidr: 100.64.0.0/16
kube_ovn_enable_mirror: true

# ------------------------ #
# ingress-controller 相关参数配置
# ------------------------ #

# 是否启用ingress-controller
ingress_controller_enabled: true

# ingress-controller类型(nginx,traefik)
ingress_controller_tpye: nginx
# NodePort svc 保留客户端的源 IP 地址，可选值：Cluster、Local
#   Cluster：不保留客户端的源 IP 地址；
#   Local：保留客户端的源 IP 地址，当设置为 Local 时仅 ingress-controller pod 运行的节点才能提供服务，其余节点不提供服务
# 相关文档：https://kubernetes.io/docs/tutorials/services/source-ip/
ingress_controller_external_traffic_policy: Cluster
# NodePort svc 监听http协议端口(注意需在NodePort端口范围)
ingress_controller_http_nodeport: 30080
# NodePort svc 监听https协议端口(注意需在NodePort端口范围)
ingress_controller_https_nodeport: 30443

# nginx-ingress 镜像地址
nginx_ingress_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/kubernetes-ingress-controller_nginx-ingress-controller:0.30.0

# traefik默认证书过期时间（天）
traefik_certs_expired: 3650
# traefik-ingress-controller 镜像地址
traefik_ingress_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/traefik:2.2.1

# ------------------------ #
# kubernetes-dashboard 相关参数配置
# ------------------------ #

# 是否启用kubernetes-dashboard 
kubernetesui_dashboard_enabled: true

# kubernetes-dashboard默认证书有效期
kubernetesui_dashboard_certs_expired: 3650

# kubernetes-dashboard 镜像地址
kubernetesui_dashboard_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/kubernetesui_dashboard:v2.0.0
kubernetesui_metrics_scraper_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/kubernetesui_metrics-scraper:v1.0.4

# ------------------------ #
# metrics-server 相关参数配置 
# ------------------------ #

# 是否启用metrics-server
metrics_server_enabled: true

# metrics-server image地址
metrics_server_image: registry.aliyuncs.com/k8sxio/metrics-server-amd64:v0.3.6

# ------------------------ #
# cert-manager 相关配置
# ------------------------ #

# 是否启用cert-manager
cert_manager_enabled: false

# acme相关配置
acme_email: yourname@gmail.com
acme_server: https://acme-v02.api.letsencrypt.org/directory

# cert-manager 相关 image 地址
cert_manager_cainjector_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/jetstack_cert-manager-cainjector:v0.9.1
cert_manager_webhook_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/jetstack_cert-manager-webhook:v0.9.1
cert_manager_controller_image: registry.cn-shanghai.aliyuncs.com/kubeadm-ha/jetstack_cert-manager-controller:v0.9.1