; 将所有节点信息在这里填写
;    第一个字段                  为远程服务器hostname，注意不能包含大写字母，如过包含请修改hostname后重启服务器生效
;    第二个字段 ansible_host     为远程服务器内网IP
;    第三个字段 ansible_user     为登录远程服务器用户名
;    第四个字段 ansible_ssh_pass 为登录远程服务器用户密码  
[all]
node1 ansible_host=192.168.56.11 ansible_user=vagrant ansible_ssh_pass=vagrant
node2 ansible_host=192.168.56.12 ansible_user=vagrant ansible_ssh_pass=vagrant
node3 ansible_host=192.168.56.13 ansible_user=vagrant ansible_ssh_pass=vagrant
node4 ansible_host=192.168.56.14 ansible_user=vagrant ansible_ssh_pass=vagrant

; 公有云：
;    一般公有云都有提供负载均衡产品，且不允许自己创建 虚拟IP 即 VIP。
;    请将lb节点组留空，仅保留组名，购买公有云 slb 服务，将 lb_mode 设置为 slb，将 lb_kube_apiserver_ip 设置为购买到的 slb 内网ip 。
;
; 私有云：
;    当 lb_mode 设置为 nginx 时，请将 所有节点! 所有节点! 所有节点! 设置到 lb 节点组中，安装nginx进行负载。
;    当 lb_mode 设置为 haproxy 时，请先与服务器提供部门协商保留一个IP作为 VIP，一般2节点就够安装 haproxy 和 keepalived 了，
;       lb节点组中第一个节点为 keepalived 的 master 节点，剩下的都为 backed 节点
[lb]

; 注意etcd集群必须是1,3,5,7...奇数个节点
[etcd]
node1
node2
node3

[kube-master]
node1

[kube-worker]
node1
node2
node3
node4

; 预留组，后续添加master节点使用
[new-master]

; 预留组，后续添加worker节点使用
[new-worker]

;-------------------------------------- 以下为基础信息配置 ------------------------------------;
[all:vars]
; 是否跳过节点物理资源校验，Master节点要求2c2g以上，Worker节点要求2c4g以上
skip_verify_node=false
; kubernetes版本
kube_version="1.14.2"
; Pod根容器
pod_infra_container_image="gcr.azk8s.cn/google_containers/pause:3.1"
; kubernetes各组件镜像仓库前缀
kube_image_repository="gcr.azk8s.cn/google_containers"
; 集群内部dns域名
kube_dns_domain="cluster.local"
; 集群pod ip段
kube_pod_subnet="10.244.0.0/18"
; 集群service ip段
kube_service_subnet="10.244.64.0/18"
; NodePort端口范围
kube_service_node_port_range="30000-32767"
; 集群网络插件，目前支持flannel,calico
network_plugin="flannel"
; Kubelet 根目录
kubelet_root_dir="/var/lib/kubelet"
; docker容器存储目录
docker_storage_dir="/var/lib/docker"