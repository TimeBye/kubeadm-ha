## 离线安装

### 约定

- 本文以 CentOS 7.4 操作系统为例进行讲解。
- 准备离线包的用户为 **root** 用户。
- 准备离线包的服务器操作系统 **全新干净**。
- 部署集群时需一台集群外的服务器用作 ansible 环境搭建。
- 准备离线包的服务器操作系统与后期部署 kubernetes 集群的服务器操作系统 **需完全一致** ，包括系统版本及内核版本。

### 准备离线包

#### 准备 rpm 安装包

0. 升级内核【可选】
    <details>

    
    > 若执行了此步骤，那么安装集群时也需进行内核升级步骤

    - 创建 `download_kernel` 文件，并粘贴以下内容
  
        ```shell
        #!/bin/bash

        # 缓存yum包
        sed "s/keepcache=0/keepcache=1/" -i /etc/yum.conf
        # 安装内核
        yum install -y \
          http://files.saas.hand-china.com/kubernetes/kernel-ml-4.20.13-1.el7.elrepo.x86_64.rpm \
          http://files.saas.hand-china.com/kubernetes/kernel-ml-devel-4.20.13-1.el7.elrepo.x86_64.rpm
        # 创建缓存目录
        mkdir -p /kubernetes
        # 下载内核rpm文件
        curl -o /kubernetes/kernel-ml-4.20.13-1.el7.elrepo.x86_64.rpm \
            http://files.saas.hand-china.com/yum/repos/kernel/kernel-ml-4.20.13-1.el7.elrepo.x86_64.rpm
        curl -o /kubernetes/kernel-ml-devel-4.20.13-1.el7.elrepo.x86_64.rpm \
            http://files.saas.hand-china.com/yum/repos/kernel/kernel-ml-devel-4.20.13-1.el7.elrepo.x86_64.rpm
        # 设置默认内核为最新版本
        grub2-set-default 0 && grub2-mkconfig -o /boot/grub2/grub.cfg
        # 获取默认内核版本
        grubby --default-kernel
        # 开启 User namespaces
        grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
        # 重启服务器
        reboot -f
        ```

    - 执行脚本
      ```shell
      bash ./download_kernel
      ```
    
    </details>

1. 离线所需 rpm 包
    ```shell  
    # 启用缓存 yum 包
    sed "s/keepcache=0/keepcache=1/" -i /etc/yum.conf
    # 安装 git
    yum install -y git
    # 克隆本项目
    git clone https://github.com/TimeBye/kubeadm-ha.git
    # 缓存 yum 包
    bash ./kubeadm-ha/offline/download_yum
    # 打包本项目
    tar -czvf /kubernetes/kubeadm-ha.tar.gz kubeadm-ha
    ```

#### 准备 helm 安装包

```shell
curl -L -o /kubernetes/helm-v2.14.3-linux-amd64.tar.gz \
    https://file.choerodon.com.cn/kubernetes-helm/v2.14.3/helm-v2.14.3-linux-amd64.tar.gz
```

#### 准备 docker 镜像离线包

> 若下载的 helm 安装包不是 v2.14.3 版本则需修改  `./kubeadm-ha/offline/download_images` 中的 tiller 镜像版本。

```shell
# 确认 docker 运行
systemctl start docker
# 缓存 docker 镜像
bash ./kubeadm-ha/offline/download_images
```

#### 缓存文件列表

- 至此打包了以下缓存文件，将以下 4 个文件上传至搭建 ansible 环境的服务器上：
  - /kubernetes/kubeadm-ha.tar.gz
  - /kubernetes/kubernetes-1.16.7.tar
  - /kubernetes/kubernetes-1.16.7.tar.gz
  - /kubernetes/helm-v2.14.3-linux-amd64.tar.gz

### 离线安装 kubernetes 集群

#### 节点信息

|    **ip**     | **hostname** |   **OS**   |      **role**      |
| :-----------: | :----------: | :--------: | :----------------: |
| 192.168.56.11 |    node1     | CentOS 7.4 | master etcd worker |
| 192.168.56.12 |    node2     | CentOS 7.4 | master etcd worker |
| 192.168.56.13 |    node3     | CentOS 7.4 | master etcd worker |
| 192.168.56.14 |    node4     | CentOS 7.4 |       worker       |
| 192.168.56.15 |    deploy    | CentOS 7.4 |      ansible       |

**注意：** 以下操作未特殊说明都在 `192.168.56.15` deploy 节点执行。

#### ansible 环境准备

- 上传准备好的 4 个缓存文件至搭建 ansible 环境的服务器上。

- 准备 ansible 环境
  ```shell
  # 解压 kubernetes-1.16.7.tar.gz
  mkdir yum
  tar -xzvf kubernetes-1.16.7.tar.gz -C yum
  # 解压 kubernetes-1.16.7.tar.gz
  tar -xzvf kubeadm-ha.tar.gz
  # 安装 docker
  yum localinstall -y ./yum/*.rpm
  # 启动 docker
  systemctl start docker
  # 加载 docker 镜像
  docker load -i kubernetes-1.16.7.tar
  ```

- 运行 ansible 镜像
  ```shell
  docker run -d --name ansible -p 12480:80 \
    -w /etc/ansible \
    -v $PWD/kubeadm-ha:/etc/ansible \
    -v $PWD/yum:/kubernetes/yum \
    -v $PWD/kubernetes-1.16.7.tar:/kubernetes/kubernetes-1.16.7.tar \
    -v $PWD/helm-v2.14.3-linux-amd64.tar.gz:/kubernetes/helm-v2.14.3-linux-amd64.tar.gz \
    -v $PWD/kubeadm-ha/offline/default.conf:/etc/nginx/conf.d/default.conf \
    dockerhub.azk8s.cn/setzero/ansible:2.8.5-nginx-1.17.6-alpine
  ```

#### 编写配置文件

- 编辑 `./kubeadm-ha/example/variables.yaml` 修改以下字段
  ```yaml
  # 设置为离线模式
  install_mode: offline
  # 所有 yum 源地址都配置为 ansible 镜像运行的服务器的IP，注意地址末尾 / 必须加上
  base_yum_repo: http://192.168.56.15:12480/yum/
  epel_yum_repo: http://192.168.56.15:12480/yum/
  docker_yum_repo: http://192.168.56.15:12480/yum/
  kubernetes_yum_repo: http://192.168.56.15:12480/yum/
  ```

- 升级内核配置（若准备离线包时升级了内核，则必须配置，否则跳过）
    <details>

    - 编辑 `./kubeadm-ha/example/variables.yaml` 追加以下字段
      ```yaml
      # 若需升级内核添加一下变量，不升级则不添加
      kernel_centos:
      - http://192.168.56.15:12480/yum/kernel-ml-4.20.13-1.el7.elrepo.x86_64.rpm
      - http://192.168.56.15:12480/yum/kernel-ml-devel-4.20.13-1.el7.elrepo.x86_64.rpm
      ```
    </details>

- 配置 ansible inventory 文件
  ```ini
  ; vi ./kubeadm-ha/inventory.ini

  ; 将所有节点的信息在这里填写
  ;    第一个字段                  为节点内网IP，部署完成后为 kubernetes 节点 nodeName
  ;    第二个字段 ansible_port     为节点 sshd 监听端口
  ;    第三个字段 ansible_user     为节点远程登录用户名
  ;    第四个字段 ansible_ssh_pass 为节点远程登录用户密码
  [all]
  192.168.56.11 ansible_port=22 ansible_user="vagrant" ansible_ssh_pass="vagrant"
  192.168.56.12 ansible_port=22 ansible_user="vagrant" ansible_ssh_pass="vagrant"
  192.168.56.13 ansible_port=22 ansible_user="vagrant" ansible_ssh_pass="vagrant"
  192.168.56.14 ansible_port=22 ansible_user="vagrant" ansible_ssh_pass="vagrant"
      
  ; 私有云：
  ;    VIP 负载模式：
  ;       也就是负载均衡器 + keepalived 模式，比如常用的 haproxy + keepalived。
  ;       本脚本中负载均衡器有 nginx、haproxy、envoy 可供选择，设置 lb_mode 即可进行任意切换。
  ;       设置 lb_kube_apiserver_ip 即表示启用 keepalived，请先与服务器提供部门协商保留一个IP作为 lb_kube_apiserver_ip，
  ;       一般 lb 节点组中有两个节点就够了，lb节点组中第一个节点为 keepalived 的 master 节点，剩下的都为 backed 节点。
  ;
  ;    节点本地负载模式：
  ;       只启动负载均衡器，不启用 keepalived（即不设置 lb_kube_apiserver_ip），
  ;       此时 kubelet 链接 apiserver 地址为 127.0.0.1:lb_kube_apiserver_port。
  ;       使用此模式时请将 lb 节点组置空。
  ;
  ; 公有云：
  ;    不推荐使用 slb 模式，建议直接使用节点本地负载模式。
  ;    若使用 slb 模式，请先使用节点本地负载模式进行部署，
  ;    部署成功后再切换至 slb 模式：
  ;       将 lb_mode 修改为 slb，将 lb_kube_apiserver_ip 设置为购买到的 slb 内网ip，
  ;       修改 lb_kube_apiserver_port 为 slb 监听端口。
  ;    再次运行初始化集群脚本即可切换至 slb 模式。
  [lb]
      
  ; 注意etcd集群必须是1,3,5,7...奇数个节点
  [etcd]
  192.168.56.11
  192.168.56.12
  192.168.56.13
      
  [kube-master]
  192.168.56.11
  192.168.56.12
  192.168.56.13
      
  [kube-worker]
  192.168.56.11
  192.168.56.12
  192.168.56.13
  192.168.56.14
      
  ; 预留组，后续添加master节点使用
  [new-master]
      
  ; 预留组，后续添加worker节点使用
  [new-worker]
      
  ; 预留组，后续添加etcd节点使用
  [new-etcd]
      
  ;-------------------------------------- 以下为基础信息配置 ------------------------------------;
  [all:vars]
  ; 是否跳过节点物理资源校验，Master节点要求2c2g以上，Worker节点要求2c4g以上
  skip_verify_node=false
  ; kubernetes版本
  kube_version="1.16.7"
  ; 负载均衡器
  ;   有 nginx、haproxy、envoy 和 slb 四个选项，默认使用 nginx；
  lb_mode="nginx"
  ; 使用负载均衡后集群 apiserver ip，设置 lb_kube_apiserver_ip 变量，则启用负载均衡器 + keepalived
  ; lb_kube_apiserver_ip="192.168.56.15"
  ; 使用负载均衡后集群 apiserver port
  lb_kube_apiserver_port="8443"
      
  ; 网段选择：pod 和 service 的网段不能与服务器网段重叠，
  ; 若有重叠请配置 `kube_pod_subnet` 和 `kube_service_subnet` 变量设置 pod 和 service 的网段，示例参考：
  ;    如果服务器网段为：10.0.0.1/8
  ;       pod 网段可设置为：192.168.0.0/18
  ;       service 网段可设置为 192.168.64.0/18
  ;    如果服务器网段为：172.16.0.1/12
  ;       pod 网段可设置为：10.244.0.0/18
  ;       service 网段可设置为 10.244.64.0/18
  ;    如果服务器网段为：192.168.0.1/16
  ;       pod 网段可设置为：10.244.0.0/18
  ;       service 网段可设置为 10.244.64.0/18
  ; 集群pod ip段
  kube_pod_subnet="10.244.0.0/18"
  ; 集群service ip段
  kube_service_subnet="10.244.64.0/18"
      
  ; 集群网络插件，目前支持flannel,calico,kube-ovn
  network_plugin="flannel"
      
  ; 若服务器磁盘分为系统盘与数据盘，请修改以下路径至数据盘自定义的目录。
  ; Kubelet 根目录
  kubelet_root_dir="/var/lib/kubelet"
  ; docker容器存储目录
  docker_storage_dir="/var/lib/docker"
  ; Etcd 数据根目录
  etcd_data_dir="/var/lib/etcd"
  ```

- 执行安装
  - 升级内核（若准备离线包时升级了内核，则必须执行，否则跳过）
    <details>

    ```
    docker exec -it ansible \
      ansible-playbook -i inventory.ini -e @example/variables.yaml 00-kernel.yml
    ```
    </details>

  - 不升级内核
    ```
    docker exec -it ansible \
      ansible-playbook -i inventory.ini -e @example/variables.yaml 90-init-cluster.yml
    ```

### 安装 helm

**注意：** 以下操作未特殊说明都在 `192.168.56.11` 第一台 master 节点执行。

#### 创建ServiceAccount

```bash
kubectl create serviceaccount --namespace kube-system helm-tiller
kubectl create clusterrolebinding helm-tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:helm-tiller
```

#### 部署客户端

1. 下载helm客户端

    ```bash
    curl -L -o helm-v2.14.3-linux-amd64.tar.gz http://192.168.56.15:12480/helm-v2.14.3-linux-amd64.tar.gz
    ```

1. 解压压缩包（以linux-amd64为例）

    ```bash
    tar -zxvf helm-v2.14.3-linux-amd64.tar.gz
    ```

1. 将文件移动到PATH目录中（以linux-amd64为例）

    ```bash
    sudo mv linux-amd64/helm /usr/bin/helm
    ```

1. 初始化Helm
  
    ```
    helm init \
      --history-max=3 \
      --tiller-image=gcr.azk8s.cn/kubernetes-helm/tiller:v2.14.3 \
      --stable-repo-url=https://mirror.azure.cn/kubernetes/charts/ \
      --service-account=helm-tiller
    ```

#### 验证部署

- 执行命令，出现以下信息即部署成功。
    
    ```console
    $ helm version
    Client: &version.Version{SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"}
    Server: &version.Version{SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"}
    ```